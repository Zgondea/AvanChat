import httpx
import json
from typing import List, Dict, Any, Optional
import numpy as np
import os
import asyncio
import torch
from transformers import AutoModelForQuestionAnswering, AutoTokenizer

class AIService:
    # Knowledge graph demo: entitÄƒÈ›i È™i relaÈ›ii juridice
    knowledge_graph = {
        "tva": {
            "cota_standard": "Cota standard de TVA Ã®n RomÃ¢nia este 19%.",
            "cote_reduse": "Cotele reduse de TVA sunt 9% È™i 5% pentru anumite bunuri È™i servicii."
        },
        "impozit": {
            "profit": "Impozitul pe profit este 16%.",
            "microÃ®ntreprinderi": "Impozitul pe venitul microÃ®ntreprinderilor este 1% sau 3% Ã®n funcÈ›ie de condiÈ›ii."
        }
    }

    @staticmethod
    def normalize_text(text):
        import re
        # NormalizeazÄƒ spaÈ›ii, eliminÄƒ caractere ciudate, corecteazÄƒ diacritice simple
        text = re.sub(r'\s+', ' ', text)
        text = text.replace('Å£', 'È›').replace('ÅŸ', 'È™').replace('Å¢', 'Èš').replace('Å', 'È˜')
        text = text.replace('â€', '"').replace('â€', '"').replace('â€™', "'")
        text = text.strip()
        return text
    def __init__(self):
        self.ollama_url = os.getenv("OLLAMA_URL", "http://localhost:11434")
        self.model_name = "llama2"
        self.romanian_model_name = "readerbench/jurBERT-base"
        self.initialized = False
        self.romanian_model = None
        self.romanian_tokenizer = None
        self.use_romanian_model = os.getenv("USE_ROMANIAN_MODEL", "true").lower() == "true"
        
    async def initialize(self):
        """Initialize the AI service components"""
        try:
            if self.use_romanian_model:
                print("ğŸ‡·ğŸ‡´ Initializing Romanian model...")
                await self._init_romanian_model()
            else:
                # Check if Ollama is running
                await self._ensure_ollama_model()
            
            self.initialized = True
            print("âœ… AI Service initialized successfully")
            
        except Exception as e:
            print(f"âŒ Failed to initialize AI Service: {e}")
            print(f"ğŸ”„ Falling back to Ollama...")
            try:
                await self._ensure_ollama_model()
                self.use_romanian_model = False
                self.initialized = True
                print("âœ… AI Service initialized with Ollama fallback")
            except Exception as fallback_error:
                print(f"âŒ Fallback also failed: {fallback_error}")
                # Continue without AI for now
    
    async def _init_romanian_model(self):
        """Initialize the Romanian model"""
        try:
            # Load model and tokenizer
            print(f"ğŸ“¥ Loading Romanian model: {self.romanian_model_name}")
            self.romanian_tokenizer = AutoTokenizer.from_pretrained(self.romanian_model_name)
            
            # Use CPU for now (can switch to GPU if available)
            device = "cuda" if torch.cuda.is_available() else "cpu"
            print(f"ğŸ–¥ï¸ Using device: {device}")
            
            self.romanian_model = AutoModelForQuestionAnswering.from_pretrained(
                self.romanian_model_name,
                torch_dtype=torch.float32,  # jurBERT works better with float32
                low_cpu_mem_usage=True
            )
            
            if device == "cpu":
                self.romanian_model = self.romanian_model.to("cpu")
            
            print("âœ… Romanian model loaded successfully")
            
        except Exception as e:
            print(f"âŒ Failed to load Romanian model: {e}")
            raise e
    
    async def _ensure_ollama_model(self):
        """Ensure the model is available in Ollama"""
        async with httpx.AsyncClient(timeout=10.0) as client:
            try:
                # Check if Ollama is running
                response = await client.get(f"{self.ollama_url}/api/tags")
                if response.status_code == 200:
                    models = response.json().get("models", [])
                    model_exists = any(model["name"].startswith(self.model_name) for model in models)
                    
                    if not model_exists:
                        print(f"âš ï¸ Model {self.model_name} not found. Please pull it manually.")
                    else:
                        print(f"âœ… Model {self.model_name} available")
                        
            except Exception as e:
                print(f"âš ï¸ Ollama not available: {e}")
    
    def generate_embedding(self, text: str) -> List[float]:
        """Generate simple embedding for text (fallback without sentence-transformers)"""
        # Simple hash-based embedding as fallback
        import hashlib
        hash_obj = hashlib.sha256(text.encode())
        hash_hex = hash_obj.hexdigest()
        
        # Convert to 384-dim vector
        embedding = []
        for i in range(0, len(hash_hex), 2):
            val = int(hash_hex[i:i+2], 16) / 255.0  # Normalize to 0-1
            embedding.append(val)
        
        # Pad or trim to 384 dimensions
        while len(embedding) < 384:
            embedding.extend(embedding[:384-len(embedding)])
        embedding = embedding[:384]
        
        return embedding
    
    async def generate_response(self, query: str, context_chunks: List[Dict[str, Any]], municipality_name: str) -> str:
        # Fallback universal pentru orice tip de Ã®ntrebare
        def fallback_fragment():
            if context_chunks:
                chunk = context_chunks[0]
                doc_name = chunk.get('document_name', 'Document necunoscut')
                page_num = chunk.get('page_number', 'N/A')
                content = self.normalize_text(chunk['content'])
                short_content = content[:300].rstrip()
                return f"â€¢ {short_content}...\n  Sursa: {doc_name}, pagina {page_num}"
            else:
                return "Nu am gÄƒsit surse relevante."
        # CÄƒutare Ã®n knowledge graph
        kg_response = None
        q_lower = query.lower()
        if "tva" in q_lower:
            if "standard" in q_lower or "cota" in q_lower:
                kg_response = self.knowledge_graph["tva"].get("cota_standard")
            elif "redus" in q_lower or "reducere" in q_lower:
                kg_response = self.knowledge_graph["tva"].get("cote_reduse")
        if "impozit" in q_lower:
            if "profit" in q_lower:
                kg_response = self.knowledge_graph["impozit"].get("profit")
            elif "micro" in q_lower:
                kg_response = self.knowledge_graph["impozit"].get("microÃ®ntreprinderi")

        # DacÄƒ gÄƒseÈ™te un fapt relevant Ã®n grafic, Ã®l returneazÄƒ augmentat cu fragmentul RAG
        # Optimizare pentru Ã®ntrebÄƒri de tip 'cum se calculeazÄƒ'
        q_lower = query.lower()
        if 'cum se calculeaza' in q_lower or 'cum se determinÄƒ' in q_lower or 'calcul' in q_lower:
            # CautÄƒ formule sau paÈ™i de calcul Ã®n fragmente
            # Extrage doar propoziÈ›ii care conÈ›in formule reale (semnul '=', cifre, procente, structurÄƒ matematicÄƒ)
            import re
            formula_pattern = re.compile(r'(\d+\s*[%=]|=|\d+\s*lei|\d+\s*RON|\d+\s*x|\d+\s*/|\d+\s*\*)')
            # Extrage cuvinte-cheie din Ã®ntrebare
            keywords = re.findall(r'\w+', query.lower())
            for chunk in context_chunks[:3]:
                content = self.normalize_text(chunk['content'])
                content = re.sub(r'\s+', ' ', content)
                content = re.sub(r'([a-zA-Z])\s+([ÄƒÃ®Ã¢È™È›Ä‚ÃÃ‚È˜Èš])', r'\1\2', content)
                content = re.sub(r'([ÄƒÃ®Ã¢È™È›Ä‚ÃÃ‚È˜Èš])\s+([a-zA-Z])', r'\1\2', content)
                content = content.replace(' .', '.').replace(' ,', ',').replace(' ;', ';')
                content = content.replace(' :', ':').replace(' ?', '?').replace(' !', '!')
                content = content.replace('â€', '"').replace('â€', '"').replace('â€™', "'")
                content = content.replace('Å£', 'È›').replace('ÅŸ', 'È™').replace('Å¢', 'Èš').replace('Å', 'È˜')
                sentences = re.split(r'(?<=[.!?])\s+', content)
                for sentence in sentences:
                    # Extrage doar dacÄƒ existÄƒ semne matematice/cifre relevante È™i cuvinte-cheie din Ã®ntrebare
                    sentence_lc = sentence.lower()
                    if formula_pattern.search(sentence) and any(kw in sentence_lc for kw in keywords):
                        doc_name = chunk.get('document_name', 'Document necunoscut')
                        page_num = chunk.get('page_number', 'N/A')
                        sentence_fmt = sentence.strip().capitalize()
                        return f"FormulÄƒ/ExplicaÈ›ie: {sentence_fmt}\nSursa: {doc_name}, pagina {page_num}"
            # DacÄƒ nu gÄƒseÈ™te formulÄƒ, returneazÄƒ ca Ã®nainte
            sources = []
            for chunk in context_chunks[:2]:
                doc_name = chunk.get('document_name', 'Document necunoscut')
                page_num = chunk.get('page_number', 'N/A')
                long_content = self.normalize_text(chunk['content'])[:500].rstrip()
                sources.append(f"â€¢ {long_content}...\n  Sursa: {doc_name}, pagina {page_num}")
            sources_text = "\n".join(sources) if sources else "Nu am gÄƒsit surse relevante."
            return f"ExplicaÈ›ie relevantÄƒ:\n{sources_text}"
        elif kg_response:
            kg_response_fmt = self.normalize_text(kg_response)
            # DacÄƒ rÄƒspunsul KG conÈ›ine un procent, returneazÄƒ doar fraza scurtÄƒ
            if "%" in kg_response_fmt:
                return kg_response_fmt
            else:
                return f"{kg_response_fmt}\n\nSurse relevante:\n{fallback_fragment()}"
        else:
            return f"Surse relevante:\n{fallback_fragment()}"
        """Generate response using Romanian model or Ollama"""
        if not self.initialized:
            return "Ne pare rÄƒu, serviciul AI nu este Ã®ncÄƒ disponibil. VÄƒ rugÄƒm sÄƒ Ã®ncercaÈ›i mai tÃ¢rziu."


        # Optimizare: selecteazÄƒ pÃ¢nÄƒ la 5 fragmente relevante pentru Ã®ntrebÄƒri fiscale
        keywords = ["tva", "procent", "cota", "impozit", "%", "taxa", "taxe", "valoare"]
        def is_relevant(chunk):
            text = chunk['content'].lower()
            has_keyword = any(kw in text for kw in keywords)
            has_digit = any(c.isdigit() for c in text)
            return has_keyword and has_digit

        # PrioritizeazÄƒ fragmente cu cuvinte cheie È™i cifre
        relevant_chunks = [chunk for chunk in context_chunks if is_relevant(chunk)]
        # DacÄƒ nu gÄƒseÈ™te nimic relevant, foloseÈ™te contextul original
        if relevant_chunks:
            context_to_use = relevant_chunks[:5]
        else:
            context_to_use = context_chunks[:5]

        # RAG simplu: returneazÄƒ direct cele mai relevante fragmente din documente
        if not context_to_use:
            return "Nu am gÄƒsit informaÈ›ii relevante Ã®n documentele disponibile pentru a rÄƒspunde la Ã®ntrebarea dumneavoastrÄƒ."

        responses = []
        for chunk in context_to_use[:3]:
            doc_name = chunk.get('document_name', 'Document necunoscut')
            page_num = chunk.get('page_number', 'N/A')
            responses.append(f"{chunk['content'].strip()}\n\n*Sursa: {doc_name}, pagina {page_num}*")

        return "\n\n---\n\n".join(responses)
    
    async def _generate_with_romanian_model(self, query: str, context_chunks: List[Dict[str, Any]], municipality_name: str) -> str:
        """Generate response using Romanian jurBERT QA model"""
        try:
            if not context_chunks:
                return "Nu am gÄƒsit informaÈ›ii relevante Ã®n documentele disponibile pentru a rÄƒspunde la Ã®ntrebarea dumneavoastrÄƒ."

            # Use the most relevant chunk as context for QA
            best_chunk = context_chunks[0]  # Already sorted by relevance
            context = best_chunk['content']

            # Prepare inputs for Question Answering
            inputs = self.romanian_tokenizer(
                query,
                context,
                add_special_tokens=True,
                return_tensors="pt",
                max_length=512,
                truncation=True,
                return_offsets_mapping=True,
                padding=True
            )

            # Get answer from model
            with torch.no_grad():
                outputs = self.romanian_model(**{k: v for k, v in inputs.items() if k != 'offset_mapping'})

            # Extract answer
            answer_start_scores = outputs.start_logits
            answer_end_scores = outputs.end_logits

            answer_start = torch.argmax(answer_start_scores)
            answer_end = torch.argmax(answer_end_scores) + 1

            # Decode answer
            answer = self.romanian_tokenizer.convert_tokens_to_string(
                self.romanian_tokenizer.convert_ids_to_tokens(inputs["input_ids"][0][answer_start:answer_end])
            )

            # Clean up the answer
            answer = answer.strip()

            # Postprocesare: dacÄƒ rÄƒspunsul nu conÈ›ine cifre sau cuvinte cheie, returneazÄƒ direct fragmentul relevant
            keywords = ["tva", "procent", "cota", "impozit", "%", "taxa", "taxe", "valoare"]
            has_keyword = any(kw in answer.lower() for kw in keywords)
            has_digit = any(c.isdigit() for c in answer)
            if not (has_keyword and has_digit):
                # CautÄƒ cel mai relevant fragment care conÈ›ine cuvinte cheie È™i cifre
                for chunk in context_chunks:
                    text = chunk['content'].lower()
                    if any(kw in text for kw in keywords) and any(c.isdigit() for c in text):
                        doc_name = chunk.get('document_name', 'Document necunoscut')
                        page_num = chunk.get('page_number', 'N/A')
                        return f"{chunk['content'].strip()}\n\n*Sursa: {doc_name}, pagina {page_num}*"

            # If answer is too short or empty, provide context-based response
            if len(answer.split()) < 3:
                # Combine information from multiple chunks
                combined_info = []
                for chunk in context_chunks[:2]:
                    doc_name = chunk.get('document_name', 'Document necunoscut')
                    page_num = chunk.get('page_number', 'N/A')
                    combined_info.append(f"Conform {doc_name} (pag. {page_num}): {chunk['content'][:200]}...")

                response = f"BazÃ¢ndu-mÄƒ pe documentele disponibile:\n\n" + "\n\n".join(combined_info)
                return response

            # Format final response with source
            doc_name = best_chunk.get('document_name', 'Document necunoscut')
            page_num = best_chunk.get('page_number', 'N/A')

            return f"{answer}\n\n*Sursa: {doc_name}, pagina {page_num}*"

        except Exception as e:
            print(f"âŒ Error with Romanian jurBERT model: {e}")
            return "Ne pare rÄƒu, am Ã®ntÃ¢mpinat o problemÄƒ tehnicÄƒ. VÄƒ rugÄƒm sÄƒ Ã®ncercaÈ›i din nou."
    
    async def _generate_with_ollama(self, query: str, context_chunks: List[Dict[str, Any]], municipality_name: str) -> str:
        """Generate response using Ollama"""
        # Prepare context
        context = "\n\n".join([
            f"Document: {chunk.get('document_name', 'Document necunoscut')}\n"
            f"Pagina: {chunk.get('page_number', 'N/A')}\n"
            f"ConÈ›inut: {chunk['content']}"
            for chunk in context_chunks[:3]
        ])

        # Prompt explicit pentru rÄƒspunsuri fiscale
        prompt = f"""
EÈ™ti un asistent AI pentru {municipality_name}. RÄƒspunde Ã®n romÃ¢nÄƒ la Ã®ntrebarea despre legislaÈ›ia fiscalÄƒ.

Context documente:
{context}

Ãntrebare: {query}

InstrucÈ›iuni speciale:
- DacÄƒ Ã®ntrebarea este despre cota de TVA, rÄƒspunde direct cu valoarea actualÄƒ (ex: Cota standard de TVA Ã®n RomÃ¢nia este 19%).
- DacÄƒ Ã®ntrebarea este despre taxe, impozite sau procente, rÄƒspunsul trebuie sÄƒ fie clar, concis È™i sÄƒ conÈ›inÄƒ valoarea numericÄƒ exactÄƒ.
- DacÄƒ nu gÄƒseÈ™ti rÄƒspunsul, spune "Nu am gÄƒsit informaÈ›ii exacte Ã®n documentele disponibile."

RÄƒspuns scurt È™i clar Ã®n romÃ¢nÄƒ:
"""

        try:
            async with httpx.AsyncClient(timeout=30.0) as client:
                response = await client.post(
                    f"{self.ollama_url}/api/generate",
                    json={
                        "model": self.model_name,
                        "prompt": prompt,
                        "stream": False,
                        "options": {"temperature": 0.3, "max_tokens": 500}
                    }
                )

                if response.status_code == 200:
                    result = response.json()
                    return result.get("response", "").strip()
                else:
                    return "Ne pare rÄƒu, nu pot genera un rÄƒspuns Ã®n acest moment."

        except Exception as e:
            print(f"âŒ Error generating response: {e}")
            return "Ne pare rÄƒu, am Ã®ntÃ¢mpinat o problemÄƒ tehnicÄƒ. VÄƒ rugÄƒm sÄƒ Ã®ncercaÈ›i din nou."
    
    def extract_sources(self, context_chunks: List[Dict[str, Any]]) -> List[Dict[str, Any]]:
        """Extract source information from context chunks"""
        sources = []
        for chunk in context_chunks:
            sources.append({
                "document_name": chunk.get('document_name', 'Document necunoscut'),
                "page_number": chunk.get('page_number', 'N/A'),
                "similarity_score": round(chunk.get('similarity_score', 0.0), 3),
                "snippet": chunk['content'][:200] + "..." if len(chunk['content']) > 200 else chunk['content']
            })
        return sources